{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Project: Chatbot\n",
    "We are making chatbot using deep learning approach.We will train the chatbot\n",
    "on dataset which has object intents, patterns and replys we are going to use LSTM\n",
    "Long Short-Term Memory (LSTM) networks are a type of recurrent neural network \n",
    "capable of learning order dependence in sequence prediction problems.To identify\n",
    "user query category and then we will sends a random reply from list according to query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk  # supports text processing like tokenization and lemmatization\n",
    "from nltk.stem import WordNetLemmatizer # WordNet is a part of Python's Natural Language Toolkit\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "import json # to operate json file/json objects\n",
    "import pickle # converts a object into a byte stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from keras.models import Sequential # for one input output\n",
    "from keras.layers import Dense, Activation, Dropout #for training\n",
    "from tensorflow.keras.optimizers import SGD #importing sgd optimizer for good accuracy\n",
    "import random #\n",
    "from nltk.tokenize import word_tokenize #for tokenizaton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "textWords=[]\n",
    "classes = []\n",
    "docs = []\n",
    "rejected = ['?', '!']\n",
    "data_file = open(r'C:\\Users\\home1\\OneDrive\\Desktop\\datasci\\intents.json').read()\n",
    "intents = json.loads(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'tag': 'greeting', 'patterns': ['Hi there', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'tag': 'goodbye', 'patterns': ['Bye', 'See yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'tag': 'thanks', 'patterns': ['Thanks', 'Than...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'tag': 'noanswer', 'patterns': [], 'responses...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'tag': 'options', 'patterns': ['How you could...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'tag': 'adverse_drug', 'patterns': ['How to c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'tag': 'blood_pressure', 'patterns': ['Open b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'tag': 'blood_pressure_search', 'patterns': [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'tag': 'search_blood_pressure_by_patient_id',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'tag': 'pharmacy_search', 'patterns': ['Find ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{'tag': 'search_pharmacy_by_name', 'patterns':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>{'tag': 'hospital_search', 'patterns': ['Looku...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'tag': 'search_hospital_by_params', 'patterns...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>{'tag': 'search_hospital_by_type', 'patterns':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>{'tag': 'xyz medicine available in pharmacy?',...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              intents\n",
       "0   {'tag': 'greeting', 'patterns': ['Hi there', '...\n",
       "1   {'tag': 'goodbye', 'patterns': ['Bye', 'See yo...\n",
       "2   {'tag': 'thanks', 'patterns': ['Thanks', 'Than...\n",
       "3   {'tag': 'noanswer', 'patterns': [], 'responses...\n",
       "4   {'tag': 'options', 'patterns': ['How you could...\n",
       "5   {'tag': 'adverse_drug', 'patterns': ['How to c...\n",
       "6   {'tag': 'blood_pressure', 'patterns': ['Open b...\n",
       "7   {'tag': 'blood_pressure_search', 'patterns': [...\n",
       "8   {'tag': 'search_blood_pressure_by_patient_id',...\n",
       "9   {'tag': 'pharmacy_search', 'patterns': ['Find ...\n",
       "10  {'tag': 'search_pharmacy_by_name', 'patterns':...\n",
       "11  {'tag': 'hospital_search', 'patterns': ['Looku...\n",
       "12  {'tag': 'search_hospital_by_params', 'patterns...\n",
       "13  {'tag': 'search_hospital_by_type', 'patterns':...\n",
       "14  {'tag': 'xyz medicine available in pharmacy?',..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_file = pd.read_json (r'C:\\Users\\home1\\OneDrive\\Desktop\\datasci\\intents.json')\n",
    "# Convert into DataFrame\n",
    "df = pd.DataFrame(data_file)\n",
    "# Display data\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "\n",
    "    \n",
    "        nw = nltk.word_tokenize(pattern) #tokenize each word \n",
    "        textWords.extend(nw)\n",
    "        \n",
    "        docs.append((nw, intent['tag'])) #add doc in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if intent['tag'] not in classes:  #add to classes lists\n",
    "    classes.append(intent['tag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#lemmatize, lower each word and remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 docs\n",
      "1 classes ['xyz medicine available in pharmacy?']\n",
      "88 unique lemmatized words [\"'s\", ',', 'a', 'adverse', 'all', 'anyone', 'are', 'awesome', 'be', 'behavior', 'blood', 'by', 'bye', 'can', 'causing', 'chatting', 'check', 'could', 'data', 'day', 'detail', 'do', 'dont', 'drug', 'entry', 'find', 'for', 'give', 'good', 'goodbye', 'have', 'hello', 'help', 'helpful', 'helping', 'hey', 'hi', 'history', 'hola', 'hospital', 'how', 'i', 'id', 'is', 'later', 'list', 'load', 'locate', 'log', 'looking', 'lookup', 'management', 'me', 'module', 'nearby', 'next', 'nice', 'of', 'offered', 'open', 'patient', 'pharmacy', 'pressure', 'provide', 'reaction', 'related', 'result', 'search', 'searching', 'see', 'show', 'suitable', 'support', 'task', 'thank', 'thanks', 'that', 'there', 'till', 'time', 'to', 'transfer', 'up', 'want', 'what', 'which', 'with', 'you']\n"
     ]
    }
   ],
   "source": [
    "textWords = [lemmatizer.lemmatize(nw.lower()) for nw in textWords if nw not in rejected] \n",
    "textWords = sorted(list(set(textWords)))\n",
    "\n",
    "classes = sorted(list(set(classes))) #sort class\n",
    "\n",
    "print (len(docs), \"docs\") #combo of intents & patterns\n",
    "\n",
    "print (len(classes), \"classes\", classes) #intents\n",
    "\n",
    "print (len(textWords), \"unique lemmatized words\", textWords) #vocabulary\n",
    "\n",
    "pickle.dump(textWords,open('textWords.pkl','wb')) #this file store the words Python object that contains a list of our vocabulary\n",
    "pickle.dump(classes,open('classes.pkl','wb')) #this file contains the list of categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#lemmatize, upper each word and remove duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 docs\n",
      "1 classes ['xyz medicine available in pharmacy?']\n",
      "88 unique lemmatized words [\"'S\", ',', 'A', 'ADVERSE', 'ALL', 'ANYONE', 'ARE', 'AWESOME', 'BE', 'BEHAVIOR', 'BLOOD', 'BY', 'BYE', 'CAN', 'CAUSING', 'CHATTING', 'CHECK', 'COULD', 'DATA', 'DAY', 'DETAIL', 'DO', 'DONT', 'DRUG', 'ENTRY', 'FIND', 'FOR', 'GIVE', 'GOOD', 'GOODBYE', 'HAVE', 'HELLO', 'HELP', 'HELPFUL', 'HELPING', 'HEY', 'HI', 'HISTORY', 'HOLA', 'HOSPITAL', 'HOW', 'I', 'ID', 'IS', 'LATER', 'LIST', 'LOAD', 'LOCATE', 'LOG', 'LOOKING', 'LOOKUP', 'MANAGEMENT', 'ME', 'MODULE', 'NEARBY', 'NEXT', 'NICE', 'OF', 'OFFERED', 'OPEN', 'PATIENT', 'PHARMACY', 'PRESSURE', 'PROVIDE', 'REACTION', 'RELATED', 'RESULT', 'SEARCH', 'SEARCHING', 'SEE', 'SHOW', 'SUITABLE', 'SUPPORT', 'TASK', 'THANK', 'THANKS', 'THAT', 'THERE', 'TILL', 'TIME', 'TO', 'TRANSFER', 'UP', 'WANT', 'WHAT', 'WHICH', 'WITH', 'YOU']\n"
     ]
    }
   ],
   "source": [
    "textWords = [lemmatizer.lemmatize(nw.upper()) for nw in textWords if nw not in rejected] \n",
    "textWords = sorted(list(set(textWords)))\n",
    "\n",
    "classes = sorted(list(set(classes))) #sort class\n",
    "\n",
    "print (len(docs), \"docs\") #combo of intents & patterns\n",
    "\n",
    "print (len(classes), \"classes\", classes) #intents\n",
    "\n",
    "print (len(textWords), \"unique lemmatized words\", textWords) #vocabulary\n",
    "\n",
    "pickle.dump(textWords,open('textWords.pkl','wb')) #this file store the words Python object that contains a list of our vocabulary\n",
    "pickle.dump(classes,open('classes.pkl','wb')) #this file contains the list of categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#lemmatize, Capitalize each word and remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 docs\n",
      "1 classes ['xyz medicine available in pharmacy?']\n",
      "88 unique lemmatized words [\"'s\", ',', 'A', 'Adverse', 'All', 'Anyone', 'Are', 'Awesome', 'Be', 'Behavior', 'Blood', 'By', 'Bye', 'Can', 'Causing', 'Chatting', 'Check', 'Could', 'Data', 'Day', 'Detail', 'Do', 'Dont', 'Drug', 'Entry', 'Find', 'For', 'Give', 'Good', 'Goodbye', 'Have', 'Hello', 'Help', 'Helpful', 'Helping', 'Hey', 'Hi', 'History', 'Hola', 'Hospital', 'How', 'I', 'Id', 'Is', 'Later', 'List', 'Load', 'Locate', 'Log', 'Looking', 'Lookup', 'Management', 'Me', 'Module', 'Nearby', 'Next', 'Nice', 'Of', 'Offered', 'Open', 'Patient', 'Pharmacy', 'Pressure', 'Provide', 'Reaction', 'Related', 'Result', 'Search', 'Searching', 'See', 'Show', 'Suitable', 'Support', 'Task', 'Thank', 'Thanks', 'That', 'There', 'Till', 'Time', 'To', 'Transfer', 'Up', 'Want', 'What', 'Which', 'With', 'You']\n"
     ]
    }
   ],
   "source": [
    "textWords = [lemmatizer.lemmatize(nw.capitalize()) for nw in textWords if nw not in rejected] \n",
    "textWords = sorted(list(set(textWords)))\n",
    "\n",
    "classes = sorted(list(set(classes))) #sort class\n",
    "\n",
    "print (len(docs), \"docs\") #combo of intents & patterns\n",
    "\n",
    "print (len(classes), \"classes\", classes) #intents\n",
    "\n",
    "print (len(textWords), \"unique lemmatized words\", textWords) #vocabulary\n",
    "\n",
    "pickle.dump(textWords,open('textWords.pkl','wb')) #this file store the words Python object that contains a list of our vocabulary\n",
    "pickle.dump(classes,open('classes.pkl','wb')) #this file contains the list of categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
